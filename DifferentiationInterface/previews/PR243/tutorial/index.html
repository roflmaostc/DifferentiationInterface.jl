<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · DifferentiationInterface.jl</title><meta name="title" content="Tutorial · DifferentiationInterface.jl"/><meta property="og:title" content="Tutorial · DifferentiationInterface.jl"/><meta property="twitter:title" content="Tutorial · DifferentiationInterface.jl"/><meta name="description" content="Documentation for DifferentiationInterface.jl."/><meta property="og:description" content="Documentation for DifferentiationInterface.jl."/><meta property="twitter:description" content="Documentation for DifferentiationInterface.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="DifferentiationInterface.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">DifferentiationInterface.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Start here</span><ul><li class="is-active"><a class="tocitem" href>Tutorial</a><ul class="internal"><li><a class="tocitem" href="#Computing-a-gradient"><span>Computing a gradient</span></a></li><li><a class="tocitem" href="#Overwriting-a-gradient"><span>Overwriting a gradient</span></a></li><li><a class="tocitem" href="#Preparing-for-multiple-gradients"><span>Preparing for multiple gradients</span></a></li><li><a class="tocitem" href="#Switching-backends"><span>Switching backends</span></a></li><li><a class="tocitem" href="#sparsity-tutorial"><span>Handling sparsity</span></a></li></ul></li><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../backends/">Backends</a></li></ul></li><li><a class="tocitem" href="../api/">API reference</a></li><li><span class="tocitem">Advanced</span><ul><li><a class="tocitem" href="../design/">Package design</a></li><li><a class="tocitem" href="../overloads/">Table of overloads</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Start here</a></li><li class="is-active"><a href>Tutorial</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tutorial</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/gdalle/DifferentiationInterface.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/gdalle/DifferentiationInterface.jl/blob/main/DifferentiationInterface/docs/src/tutorial.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Tutorial"><a class="docs-heading-anchor" href="#Tutorial">Tutorial</a><a id="Tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#Tutorial" title="Permalink"></a></h1><p>We present a typical workflow with DifferentiationInterface.jl and showcase its potential performance benefits.</p><pre><code class="language-julia hljs">using DifferentiationInterface</code></pre><h2 id="Computing-a-gradient"><a class="docs-heading-anchor" href="#Computing-a-gradient">Computing a gradient</a><a id="Computing-a-gradient-1"></a><a class="docs-heading-anchor-permalink" href="#Computing-a-gradient" title="Permalink"></a></h2><p>A common use case of automatic differentiation (AD) is optimizing real-valued functions with first- or second-order methods. Let&#39;s define a simple objective and a random input vector</p><pre><code class="language-julia hljs">f(x) = sum(abs2, x)

x = collect(1.0:5.0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float64}:
 1.0
 2.0
 3.0
 4.0
 5.0</code></pre><p>To compute its gradient, we need to choose a &quot;backend&quot;, i.e. an AD package to call under the hood. Most backend types are defined by <a href="https://github.com/SciML/ADTypes.jl">ADTypes.jl</a> and re-exported by DifferentiationInterface.jl.</p><p><a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a> is very generic and efficient for low-dimensional inputs, so it&#39;s a good starting point:</p><pre><code class="language-julia hljs">import ForwardDiff

backend = AutoForwardDiff()</code></pre><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>To avoid name conflicts, load AD packages with <code>import</code> instead of <code>using</code>. Indeed, most AD packages also export operators like <code>gradient</code> and <code>jacobian</code>, but you only want to use the ones from DifferentiationInterface.jl.</p></div></div><p>Now you can use the following syntax to compute the gradient:</p><pre><code class="language-julia hljs">gradient(f, backend, x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float64}:
  2.0
  4.0
  6.0
  8.0
 10.0</code></pre><p>Was that fast? <a href="https://github.com/JuliaCI/BenchmarkTools.jl">BenchmarkTools.jl</a> helps you answer that question.</p><pre><code class="language-julia hljs">using BenchmarkTools

@benchmark gradient($f, $backend, $x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 182 evaluations.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">577.511 ns</span></span> … <span class="sgr35"> 32.794 μs</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 97.53%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">597.495 ns               </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">633.281 ns</span></span> ± <span class="sgr32">587.490 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>3.83% ±  4.57%

  ▂▅██<span class="sgr34">▇</span>▇▅▄▂  <span class="sgr32"> </span>                                                  ▂
  ████<span class="sgr34">█</span>█████▇<span class="sgr32">▆</span>▇███▇▇▇▆▆▆▆▅▆▅▄▅▅▅▅▅▂▄▅▄▅▂▂▃▄▃▂▄▄▂▄▄▄▃▃▄▄▄▅▄▆▅▇▇▆ █
  578 ns<span class="sgr90">        Histogram: <span class="sgr1">log(</span>frequency<span class="sgr1">)</span> by time</span>        900 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">848 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">4</span>.</code></pre><p>Not bad, but you can do better.</p><h2 id="Overwriting-a-gradient"><a class="docs-heading-anchor" href="#Overwriting-a-gradient">Overwriting a gradient</a><a id="Overwriting-a-gradient-1"></a><a class="docs-heading-anchor-permalink" href="#Overwriting-a-gradient" title="Permalink"></a></h2><p>Since you know how much space your gradient will occupy (the same as your input <code>x</code>), you can pre-allocate that memory and offer it to AD. Some backends get a speed boost from this trick.</p><pre><code class="language-julia hljs">grad = similar(x)
gradient!(f, grad, backend, x)
grad  # has been mutated</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float64}:
  2.0
  4.0
  6.0
  8.0
 10.0</code></pre><p>The bang indicates that one of the arguments of <code>gradient!</code> might be mutated. More precisely, our convention is that <em>every positional argument between the function and the backend is mutated (and the <code>extras</code> too, see below)</em>.</p><pre><code class="language-julia hljs">@benchmark gradient!($f, _grad, $backend, $x) evals=1 setup=(_grad=similar($x))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">581.000 ns</span></span> … <span class="sgr35"> 12.243 μs</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 0.00%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">621.000 ns               </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">645.214 ns</span></span> ± <span class="sgr32">175.473 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>0.00% ± 0.00%

   ▄ ▇█▅█<span class="sgr34"> </span>▆▄▂<span class="sgr32">▃</span> ▂▁ ▂ ▃▃ ▂ ▂▂ ▁  ▁ ▁▁ ▂ ▂▁ ▂ ▁▁                   ▂
  ▆█▁████<span class="sgr34">▁</span>███<span class="sgr32">█</span>▁██▁█▁██▁█▁██▁██▇█▁████▁██▁█▁██▁█▁█▇▁█▅▆▇▁▇▄▆▇▁▆▅ █
  581 ns<span class="sgr90">        Histogram: <span class="sgr1">log(</span>frequency<span class="sgr1">)</span> by time</span>        952 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">752 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">3</span>.</code></pre><p>For some reason the in-place version is not much better than your first attempt. However, it makes fewer allocations, thanks to the gradient vector you provided. Don&#39;t worry, you can get even more performance.</p><h2 id="Preparing-for-multiple-gradients"><a class="docs-heading-anchor" href="#Preparing-for-multiple-gradients">Preparing for multiple gradients</a><a id="Preparing-for-multiple-gradients-1"></a><a class="docs-heading-anchor-permalink" href="#Preparing-for-multiple-gradients" title="Permalink"></a></h2><p>Internally, ForwardDiff.jl creates some data structures to keep track of things. These objects can be reused between gradient computations, even on different input values. We abstract away the preparation step behind a backend-agnostic syntax:</p><pre><code class="language-julia hljs">extras = prepare_gradient(f, backend, x)</code></pre><p>You don&#39;t need to know what this object is, you just need to pass it to the gradient operator.</p><pre><code class="language-julia hljs">grad = similar(x)
gradient!(f, grad, backend, x, extras)
grad  # has been mutated</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float64}:
  2.0
  4.0
  6.0
  8.0
 10.0</code></pre><p>Preparation makes the gradient computation much faster, and (in this case) allocation-free.</p><pre><code class="language-julia hljs">@benchmark gradient!($f, _grad, $backend, $x, _extras) evals=1 setup=(
    _grad=similar($x);
    _extras=prepare_gradient($f, $backend, $x)
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">69.000 ns</span></span> … <span class="sgr35">921.000 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 0.00%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">80.000 ns               </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">86.254 ns</span></span> ± <span class="sgr32"> 31.537 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>0.00% ± 0.00%

  ▇    <span class="sgr34">█</span>   <span class="sgr32"> </span>▅    ▂                        ▂    ▃    ▂    ▁     ▁
  █▁▁▁▁<span class="sgr34">█</span>▁▁▁<span class="sgr32">▇</span>█▁▁▁▁█▁▁▁▁▇▁▁▁▁▆▁▁▁▁▅▁▁▁▁▆▁▁▁▁█▁▁▁▁█▁▁▁▁█▁▁▁▁█▁▁▁▅ █
  69 ns<span class="sgr90">         Histogram: <span class="sgr1">log(</span>frequency<span class="sgr1">)</span> by time</span>       190 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">0 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">0</span>.</code></pre><p>Beware that the <code>extras</code> object is nearly always mutated by differentiation operators, even though it is given as the last positional argument.</p><h2 id="Switching-backends"><a class="docs-heading-anchor" href="#Switching-backends">Switching backends</a><a id="Switching-backends-1"></a><a class="docs-heading-anchor-permalink" href="#Switching-backends" title="Permalink"></a></h2><p>The whole point of DifferentiationInterface.jl is that you can easily experiment with different AD solutions. Typically, for gradients, reverse mode AD might be a better fit, so let&#39;s try <a href="https://github.com/JuliaDiff/ReverseDiff.jl">ReverseDiff.jl</a>!</p><p>For this one, the backend definition is slightly more involved, because you can specify whether the tape needs to be compiled:</p><pre><code class="language-julia hljs">import ReverseDiff

backend2 = AutoReverseDiff(; compile=true)</code></pre><p>But once it is done, things run smoothly with exactly the same syntax:</p><pre><code class="language-julia hljs">gradient(f, backend2, x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float64}:
  2.0
  4.0
  6.0
  8.0
 10.0</code></pre><p>And you can run the same benchmarks to see what you gained (although such a small input may not be realistic):</p><pre><code class="language-julia hljs">@benchmark gradient!($f, _grad, $backend2, $x, _extras) evals=1 setup=(
    _grad=similar($x);
    _extras=prepare_gradient($f, $backend2, $x)
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BenchmarkTools.Trial: 10000 samples with 1 evaluation.
 Range <span class="sgr90">(</span><span class="sgr36"><span class="sgr1">min</span></span> … <span class="sgr35">max</span><span class="sgr90">):  </span><span class="sgr36"><span class="sgr1">270.000 ns</span></span> … <span class="sgr35">751.000 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>min … max<span class="sgr90">): </span>0.00% … 0.00%
 Time  <span class="sgr90">(</span><span class="sgr34"><span class="sgr1">median</span></span><span class="sgr90">):     </span><span class="sgr34"><span class="sgr1">291.000 ns               </span></span><span class="sgr90">┊</span> GC <span class="sgr90">(</span>median<span class="sgr90">):    </span>0.00%
 Time  <span class="sgr90">(</span><span class="sgr32"><span class="sgr1">mean</span></span> ± <span class="sgr32">σ</span><span class="sgr90">):   </span><span class="sgr32"><span class="sgr1">295.365 ns</span></span> ± <span class="sgr32"> 19.352 ns</span>  <span class="sgr90">┊</span> GC <span class="sgr90">(</span>mean ± σ<span class="sgr90">):  </span>0.00% ± 0.00%

        ▆     █<span class="sgr34"> </span> <span class="sgr32"> </span>  ▆     ▃▃    ▁▂     ▁                        ▂
  ▄▁▁▁▁▁█▁▁▁▁▁█<span class="sgr34">▁</span>▁<span class="sgr32">▁</span>▁▁█▁▁▁▁▁██▁▁▁▁██▁▁▁▁██▁▁▁▁▇█▁▁▁▁▇█▁▁▁▁▇█▁▁▁▁▄ █
  270 ns<span class="sgr90">        Histogram: <span class="sgr1">log(</span>frequency<span class="sgr1">)</span> by time</span>        370 ns <span class="sgr1">&lt;</span>

 Memory estimate<span class="sgr90">: </span><span class="sgr33">0 bytes</span>, allocs estimate<span class="sgr90">: </span><span class="sgr33">0</span>.</code></pre><p>In short, DifferentiationInterface.jl allows for easy testing and comparison of AD backends. If you want to go further, check out the <a href="https://gdalle.github.io/DifferentiationInterface.jl/DifferentiationInterfaceTest/dev/tutorial/">DifferentiationInterfaceTest.jl tutorial</a>. It provides benchmarking utilities to compare backends and help you select the one that is best suited for your problem.</p><h2 id="sparsity-tutorial"><a class="docs-heading-anchor" href="#sparsity-tutorial">Handling sparsity</a><a id="sparsity-tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#sparsity-tutorial" title="Permalink"></a></h2><p>To compute sparse Jacobians or Hessians, you need three ingredients (read <a href="https://epubs.siam.org/doi/10.1137/S0036144504444711">this survey</a> to understand why):</p><ol><li>A sparsity pattern detector</li><li>A coloring algorithm</li><li>An underlying AD backend</li></ol><p>ADTypes.jl v1.0 defines the <a href="../backends/#ADTypes.AutoSparse"><code>AutoSparse</code></a> wrapper, which brings together these three ingredients. At the moment, this new wrapper is not well-supported in the ecosystem, which is why DifferentiationInterface.jl provides the necessary objects to get you started:</p><ol><li><a href="../api/#DifferentiationInterface.SymbolicsSparsityDetector"><code>DifferentiationInterface.SymbolicsSparsityDetector</code></a> (requires <a href="https://github.com/JuliaSymbolics/Symbolics.jl">Symbolics.jl</a> to be loaded)</li><li><a href="../api/#DifferentiationInterface.GreedyColoringAlgorithm"><code>DifferentiationInterface.GreedyColoringAlgorithm</code></a></li></ol><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>These objects are not part of the public API, so they can change unexpectedly between versions.</p></div></div><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>The symbolic backends have built-in sparsity handling, so <code>AutoSparse(AutoSymbolics())</code> and <code>AutoSparse(AutoFastDifferentiation())</code> do not need additional configuration.</p></div></div><p>Here&#39;s an example:</p><pre><code class="language-julia hljs">import Symbolics

sparsity_detector = DifferentiationInterface.SymbolicsSparsityDetector()
coloring_algorithm = DifferentiationInterface.GreedyColoringAlgorithm()
dense_backend = AutoForwardDiff()

sparse_backend = AutoSparse(dense_backend; sparsity_detector, coloring_algorithm)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">AutoSparse{AutoForwardDiff{nothing, Nothing}, DifferentiationInterface.SymbolicsSparsityDetector, DifferentiationInterface.GreedyColoringAlgorithm}(AutoForwardDiff{nothing, Nothing}(nothing), DifferentiationInterface.SymbolicsSparsityDetector(), DifferentiationInterface.GreedyColoringAlgorithm())</code></pre><p>See how the computed Hessian is sparse, whereas the underlying backend alone would give us a dense matrix:</p><pre><code class="language-julia hljs">hessian(f, sparse_backend, x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5×5 SparseArrays.SparseMatrixCSC{Float64, Int64} with 5 stored entries:
 2.0   ⋅    ⋅    ⋅    ⋅ 
  ⋅   2.0   ⋅    ⋅    ⋅ 
  ⋅    ⋅   2.0   ⋅    ⋅ 
  ⋅    ⋅    ⋅   2.0   ⋅ 
  ⋅    ⋅    ⋅    ⋅   2.0</code></pre><pre><code class="language-julia hljs">hessian(f, dense_backend, x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5×5 Matrix{Float64}:
 2.0  0.0  0.0  0.0  0.0
 0.0  2.0  0.0  0.0  0.0
 0.0  0.0  2.0  0.0  0.0
 0.0  0.0  0.0  2.0  0.0
 0.0  0.0  0.0  0.0  2.0</code></pre><p>The sparsity detector and coloring algorithm are called during the preparation step, which can be fairly expensive. If you plan to compute several Jacobians or Hessians with the same pattern but different input vectors, you should reuse the <code>extras</code> object created by <code>prepare_jacobian</code> or <code>prepare_hessian</code>. After preparation, the sparse computation itself will be much faster than the dense one, and require fewer calls to the function. </p><script type="module">import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
mermaid.initialize({
    startOnLoad: true,
    theme: "neutral"
});
</script></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../overview/">Overview »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Monday 6 May 2024 15:54">Monday 6 May 2024</span>. Using Julia version 1.10.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
