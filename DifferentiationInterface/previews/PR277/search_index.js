var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/","page":"API","title":"API","text":"DifferentiationInterface","category":"page"},{"location":"api/#DifferentiationInterface","page":"API","title":"DifferentiationInterface","text":"DifferentiationInterface\n\nAn interface to various automatic differentiation backends in Julia.\n\nExports\n\nAutoChainRules\nAutoDiffractor\nAutoEnzyme\nAutoFastDifferentiation\nAutoFiniteDiff\nAutoFiniteDifferences\nAutoForwardDiff\nAutoPolyesterForwardDiff\nAutoReverseDiff\nAutoSparse\nAutoSymbolics\nAutoTapir\nAutoTracker\nAutoZygote\nDifferentiateWith\nGreedyColoringAlgorithm\nSecondOrder\ncheck_available\ncheck_hessian\ncheck_twoarg\nderivative\nderivative!\ngradient\ngradient!\nhessian\nhessian!\nhvp\nhvp!\njacobian\njacobian!\nprepare_derivative\nprepare_gradient\nprepare_hessian\nprepare_hvp\nprepare_hvp_same_point\nprepare_jacobian\nprepare_pullback\nprepare_pullback_same_point\nprepare_pushforward\nprepare_pushforward_same_point\nprepare_second_derivative\npullback\npullback!\npushforward\npushforward!\nsecond_derivative\nsecond_derivative!\nvalue_and_derivative\nvalue_and_derivative!\nvalue_and_gradient\nvalue_and_gradient!\nvalue_and_jacobian\nvalue_and_jacobian!\nvalue_and_pullback\nvalue_and_pullback!\nvalue_and_pushforward\nvalue_and_pushforward!\n\n\n\n\n\n","category":"module"},{"location":"api/#First-order","page":"API","title":"First order","text":"","category":"section"},{"location":"api/#Pushforward","page":"API","title":"Pushforward","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_pushforward\nprepare_pushforward_same_point\npushforward\npushforward!\nvalue_and_pushforward\nvalue_and_pushforward!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_pushforward","page":"API","title":"DifferentiationInterface.prepare_pushforward","text":"prepare_pushforward(f,     backend, x, dx) -> extras\nprepare_pushforward(f!, y, backend, x, dx) -> extras\n\nCreate an extras object that can be given to pushforward and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again. In the two-argument case, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare_pushforward_same_point","page":"API","title":"DifferentiationInterface.prepare_pushforward_same_point","text":"prepare_pushforward_same_point(f,     backend, x, dx) -> extras_same\nprepare_pushforward_same_point(f!, y, backend, x, dx) -> extras_same\n\nCreate an extras_same object that can be given to pushforward and its variants if they are applied at the same point x.\n\nwarning: Warning\nIf the function or the point changes in any way, the result of preparation will be invalidated, and you will need to run it again. In the two-argument case, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pushforward","page":"API","title":"DifferentiationInterface.pushforward","text":"pushforward(f,     backend, x, dx, [extras]) -> dy\npushforward(f!, y, backend, x, dx, [extras]) -> dy\n\nCompute the pushforward of the function f at point x with seed dx.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pushforward!","page":"API","title":"DifferentiationInterface.pushforward!","text":"pushforward!(f,     dy, backend, x, dx, [extras]) -> dy\npushforward!(f!, y, dy, backend, x, dx, [extras]) -> dy\n\nCompute the pushforward of the function f at point x with seed dx, overwriting dy.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pushforward","page":"API","title":"DifferentiationInterface.value_and_pushforward","text":"value_and_pushforward(f,     backend, x, dx, [extras]) -> (y, dy)\nvalue_and_pushforward(f!, y, backend, x, dx, [extras]) -> (y, dy)\n\nCompute the value and the pushforward of the function f at point x with seed dx.\n\ninfo: Info\nRequired primitive for forward mode backends.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pushforward!","page":"API","title":"DifferentiationInterface.value_and_pushforward!","text":"value_and_pushforward!(f,     dy, backend, x, dx, [extras]) -> (y, dy)\nvalue_and_pushforward!(f!, y, dy, backend, x, dx, [extras]) -> (y, dy)\n\nCompute the value and the pushforward of the function f at point x with seed dx, overwriting dy.\n\n\n\n\n\n","category":"function"},{"location":"api/#Pullback","page":"API","title":"Pullback","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_pullback\nprepare_pullback_same_point\npullback\npullback!\nvalue_and_pullback\nvalue_and_pullback!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_pullback","page":"API","title":"DifferentiationInterface.prepare_pullback","text":"prepare_pullback(f,     backend, x, dy) -> extras\nprepare_pullback(f!, y, backend, x, dy) -> extras\n\nCreate an extras object that can be given to pullback and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again. In the two-argument case, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare_pullback_same_point","page":"API","title":"DifferentiationInterface.prepare_pullback_same_point","text":"prepare_pullback_same_point(f,     backend, x, dy) -> extras_same\nprepare_pullback_same_point(f!, y, backend, x, dy) -> extras_same\n\nCreate an extras_same object that can be given to pullback and its variants if they are applied at the same point x.\n\nwarning: Warning\nIf the function or the point changes in any way, the result of preparation will be invalidated, and you will need to run it again. In the two-argument case, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pullback","page":"API","title":"DifferentiationInterface.pullback","text":"pullback(f,     backend, x, dy, [extras]) -> dx\npullback(f!, y, backend, x, dy, [extras]) -> dx\n\nCompute the pullback of the function f at point x with seed dy.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pullback!","page":"API","title":"DifferentiationInterface.pullback!","text":"pullback!(f,     dx, backend, x, dy, [extras]) -> dx\npullback!(f!, y, dx, backend, x, dy, [extras]) -> dx\n\nCompute the pullback of the function f at point x with seed dy, overwriting dx.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pullback","page":"API","title":"DifferentiationInterface.value_and_pullback","text":"value_and_pullback(f,     backend, x, dy, [extras]) -> (y, dx)\nvalue_and_pullback(f!, y, backend, x, dy, [extras]) -> (y, dx)\n\nCompute the value and the pullback of the function f at point x with seed dy.\n\ninfo: Info\nRequired primitive for reverse mode backends.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pullback!","page":"API","title":"DifferentiationInterface.value_and_pullback!","text":"value_and_pullback!(f,     dx, backend, x, dy, [extras]) -> (y, dx)\nvalue_and_pullback!(f!, y, dx, backend, x, dy, [extras]) -> (y, dx)\n\nCompute the value and the pullback of the function f at point x with seed dy, overwriting dx.\n\n\n\n\n\n","category":"function"},{"location":"api/#Derivative","page":"API","title":"Derivative","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_derivative\nderivative\nderivative!\nvalue_and_derivative\nvalue_and_derivative!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_derivative","page":"API","title":"DifferentiationInterface.prepare_derivative","text":"prepare_derivative(f,     backend, x) -> extras\nprepare_derivative(f!, y, backend, x) -> extras\n\nCreate an extras object that can be given to derivative and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again. In the two-argument case, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.derivative","page":"API","title":"DifferentiationInterface.derivative","text":"derivative(f,     backend, x, [extras]) -> der\nderivative(f!, y, backend, x, [extras]) -> der\n\nCompute the derivative of the function f at point x.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.derivative!","page":"API","title":"DifferentiationInterface.derivative!","text":"derivative!(f,     der, backend, x, [extras]) -> der\nderivative!(f!, y, der, backend, x, [extras]) -> der\n\nCompute the derivative of the function f at point x, overwriting der.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_derivative","page":"API","title":"DifferentiationInterface.value_and_derivative","text":"value_and_derivative(f,     backend, x, [extras]) -> (y, der)\nvalue_and_derivative(f!, y, backend, x, [extras]) -> (y, der)\n\nCompute the value and the derivative of the function f at point x.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_derivative!","page":"API","title":"DifferentiationInterface.value_and_derivative!","text":"value_and_derivative!(f,     der, backend, x, [extras]) -> (y, der)\nvalue_and_derivative!(f!, y, der, backend, x, [extras]) -> (y, der)\n\nCompute the value and the derivative of the function f at point x, overwriting der.\n\n\n\n\n\n","category":"function"},{"location":"api/#Gradient","page":"API","title":"Gradient","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_gradient\ngradient\ngradient!\nvalue_and_gradient\nvalue_and_gradient!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_gradient","page":"API","title":"DifferentiationInterface.prepare_gradient","text":"prepare_gradient(f, backend, x) -> extras\n\nCreate an extras object that can be given to gradient and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.gradient","page":"API","title":"DifferentiationInterface.gradient","text":"gradient(f, backend, x, [extras]) -> grad\n\nCompute the gradient of the function f at point x.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.gradient!","page":"API","title":"DifferentiationInterface.gradient!","text":"gradient!(f, grad, backend, x, [extras]) -> grad\n\nCompute the gradient of the function f at point x, overwriting grad.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_gradient","page":"API","title":"DifferentiationInterface.value_and_gradient","text":"value_and_gradient(f, backend, x, [extras]) -> (y, grad)\n\nCompute the value and the gradient of the function f at point x.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_gradient!","page":"API","title":"DifferentiationInterface.value_and_gradient!","text":"value_and_gradient!(f, grad, backend, x, [extras]) -> (y, grad)\n\nCompute the value and the gradient of the function f at point x, overwriting grad.\n\n\n\n\n\n","category":"function"},{"location":"api/#Jacobian","page":"API","title":"Jacobian","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_jacobian\njacobian\njacobian!\nvalue_and_jacobian\nvalue_and_jacobian!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_jacobian","page":"API","title":"DifferentiationInterface.prepare_jacobian","text":"prepare_jacobian(f,     backend, x) -> extras\nprepare_jacobian(f!, y, backend, x) -> extras\n\nCreate an extras object that can be given to jacobian and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again. In the two-argument case, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.jacobian","page":"API","title":"DifferentiationInterface.jacobian","text":"jacobian(f,     backend, x, [extras]) -> jac\njacobian(f!, y, backend, x, [extras]) -> jac\n\nCompute the Jacobian matrix of the function f at point x.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.jacobian!","page":"API","title":"DifferentiationInterface.jacobian!","text":"jacobian!(f,     jac, backend, x, [extras]) -> jac\njacobian!(f!, y, jac, backend, x, [extras]) -> jac\n\nCompute the Jacobian matrix of the function f at point x, overwriting jac.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_jacobian","page":"API","title":"DifferentiationInterface.value_and_jacobian","text":"value_and_jacobian(f,     backend, x, [extras]) -> (y, jac)\nvalue_and_jacobian(f!, y, backend, x, [extras]) -> (y, jac)\n\nCompute the value and the Jacobian matrix of the function f at point x.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_jacobian!","page":"API","title":"DifferentiationInterface.value_and_jacobian!","text":"value_and_jacobian!(f,     jac, backend, x, [extras]) -> (y, jac)\nvalue_and_jacobian!(f!, y, jac, backend, x, [extras]) -> (y, jac)\n\nCompute the value and the Jacobian matrix of the function f at point x, overwriting jac.\n\n\n\n\n\n","category":"function"},{"location":"api/#Second-order","page":"API","title":"Second order","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"SecondOrder","category":"page"},{"location":"api/#DifferentiationInterface.SecondOrder","page":"API","title":"DifferentiationInterface.SecondOrder","text":"SecondOrder\n\nCombination of two backends for second-order differentiation.\n\ndanger: Danger\nSecondOrder backends do not support first-order operators.\n\nConstructor\n\nSecondOrder(outer_backend, inner_backend)\n\nFields\n\nouter::ADTypes.AbstractADType: backend for the outer differentiation\ninner::ADTypes.AbstractADType: backend for the inner differentiation\n\n\n\n\n\n","category":"type"},{"location":"api/#Second-derivative","page":"API","title":"Second derivative","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_second_derivative\nsecond_derivative\nsecond_derivative!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_second_derivative","page":"API","title":"DifferentiationInterface.prepare_second_derivative","text":"prepare_second_derivative(f, backend, x) -> extras\n\nCreate an extras object that can be given to second_derivative and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.second_derivative","page":"API","title":"DifferentiationInterface.second_derivative","text":"second_derivative(f, backend, x, [extras]) -> der2\n\nCompute the second derivative of the function f at point x.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.second_derivative!","page":"API","title":"DifferentiationInterface.second_derivative!","text":"second_derivative!(f, der2, backend, x, [extras]) -> der2\n\nCompute the second derivative of the function f at point x, overwriting der2.\n\n\n\n\n\n","category":"function"},{"location":"api/#Hessian-vector-product","page":"API","title":"Hessian-vector product","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_hvp\nprepare_hvp_same_point\nhvp\nhvp!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_hvp","page":"API","title":"DifferentiationInterface.prepare_hvp","text":"prepare_hvp(f, backend, x, v) -> extras\n\nCreate an extras object that can be given to hvp and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare_hvp_same_point","page":"API","title":"DifferentiationInterface.prepare_hvp_same_point","text":"prepare_hvp_same_point(f, backend, x, v) -> extras_same\n\nCreate an extras_same object that can be given to hvp and its variants if they are applied at the same point x.\n\nwarning: Warning\nIf the function or the point changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hvp","page":"API","title":"DifferentiationInterface.hvp","text":"hvp(f, backend, x, v, [extras]) -> p\n\nCompute the Hessian-vector product of f at point x with seed v.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hvp!","page":"API","title":"DifferentiationInterface.hvp!","text":"hvp!(f, p, backend, x, v, [extras]) -> p\n\nCompute the Hessian-vector product of f at point x with seed v, overwriting p.\n\n\n\n\n\n","category":"function"},{"location":"api/#Hessian","page":"API","title":"Hessian","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_hessian\nhessian\nhessian!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_hessian","page":"API","title":"DifferentiationInterface.prepare_hessian","text":"prepare_hessian(f, backend, x) -> extras\n\nCreate an extras object that can be given to hessian and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hessian","page":"API","title":"DifferentiationInterface.hessian","text":"hessian(f, backend, x, [extras]) -> hess\n\nCompute the Hessian matrix of the function f at point x.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hessian!","page":"API","title":"DifferentiationInterface.hessian!","text":"hessian!(f, hess, backend, x, [extras]) -> hess\n\nCompute the Hessian matrix of the function f at point x, overwriting hess.\n\n\n\n\n\n","category":"function"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/#Backend-queries","page":"API","title":"Backend queries","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"check_available\ncheck_twoarg\ncheck_hessian\nDifferentiationInterface.outer\nDifferentiationInterface.inner","category":"page"},{"location":"api/#DifferentiationInterface.check_available","page":"API","title":"DifferentiationInterface.check_available","text":"check_available(backend)\n\nCheck whether backend is available (i.e. whether the extension is loaded).\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.check_twoarg","page":"API","title":"DifferentiationInterface.check_twoarg","text":"check_twoarg(backend)\n\nCheck whether backend supports differentiation of two-argument functions.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.check_hessian","page":"API","title":"DifferentiationInterface.check_hessian","text":"check_hessian(backend)\n\nCheck whether backend supports second order differentiation by trying to compute a hessian.\n\nwarning: Warning\nMight take a while due to compilation time.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.outer","page":"API","title":"DifferentiationInterface.outer","text":"outer(backend::SecondOrder)\n\nReturn the outer backend of a SecondOrder object, tasked with differentiation at the second order.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.inner","page":"API","title":"DifferentiationInterface.inner","text":"inner(backend::SecondOrder)\n\nReturn the inner backend of a SecondOrder object, tasked with differentiation at the first order.\n\n\n\n\n\n","category":"function"},{"location":"api/#Backend-switch","page":"API","title":"Backend switch","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DifferentiateWith","category":"page"},{"location":"api/#DifferentiationInterface.DifferentiateWith","page":"API","title":"DifferentiationInterface.DifferentiateWith","text":"DifferentiateWith\n\nCallable function wrapper that enforces differentiation with a specified (inner) backend.\n\nThis works by defining new rules overriding the behavior of the outer backend that would normally be used.\n\nwarning: Warning\nThis is an experimental functionality, whose API cannot yet be considered stable. At the moment, it only supports one-argument functions, and rules are only defined for ChainRules.jl-compatible outer backends.\n\nFields\n\nf: the function in question\nbackend::AbstractADType: the inner backend to use for differentiation\n\nConstructor\n\nDifferentiateWith(f, backend)\n\nExample\n\nusing DifferentiationInterface\nimport ForwardDiff, Zygote\n\nfunction f(x)\n    a = Vector{eltype(x)}(undef, 1)\n    a[1] = sum(x)  # mutation that breaks Zygote\n    return a[1]\nend\n\ndw = DifferentiateWith(f, AutoForwardDiff());\n\ngradient(dw, AutoZygote(), [2.0])  # calls ForwardDiff instead\n\n# output\n\n1-element Vector{Float64}:\n 1.0\n\n\n\n\n\n","category":"type"},{"location":"api/#Internals","page":"API","title":"Internals","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"The following is not part of the public API.","category":"page"},{"location":"api/","page":"API","title":"API","text":"Modules = [DifferentiationInterface]\nPublic = false\nFilter = t -> !(Symbol(t) in [:outer, :inner])","category":"page"},{"location":"api/#DifferentiationInterface.DerivativeExtras","page":"API","title":"DifferentiationInterface.DerivativeExtras","text":"DerivativeExtras\n\nAbstract type for additional information needed by derivative and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ForwardOverForward","page":"API","title":"DifferentiationInterface.ForwardOverForward","text":"ForwardOverForward\n\nTraits identifying second-order backends that compute HVPs in forward over forward mode (inefficient).\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ForwardOverReverse","page":"API","title":"DifferentiationInterface.ForwardOverReverse","text":"ForwardOverReverse\n\nTraits identifying second-order backends that compute HVPs in forward over reverse mode.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.GradientExtras","page":"API","title":"DifferentiationInterface.GradientExtras","text":"GradientExtras\n\nAbstract type for additional information needed by gradient and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.HVPExtras","page":"API","title":"DifferentiationInterface.HVPExtras","text":"HVPExtras\n\nAbstract type for additional information needed by hvp and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.HessianExtras","page":"API","title":"DifferentiationInterface.HessianExtras","text":"HessianExtras\n\nAbstract type for additional information needed by hessian and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.JacobianExtras","page":"API","title":"DifferentiationInterface.JacobianExtras","text":"JacobianExtras\n\nAbstract type for additional information needed by jacobian and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PullbackExtras","page":"API","title":"DifferentiationInterface.PullbackExtras","text":"PullbackExtras\n\nAbstract type for additional information needed by pullback and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PullbackFast","page":"API","title":"DifferentiationInterface.PullbackFast","text":"PullbackFast\n\nTrait identifying backends that support efficient pullbacks.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PullbackSlow","page":"API","title":"DifferentiationInterface.PullbackSlow","text":"PullbackSlow\n\nTrait identifying backends that do not support efficient pullbacks.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PushforwardExtras","page":"API","title":"DifferentiationInterface.PushforwardExtras","text":"PushforwardExtras\n\nAbstract type for additional information needed by pushforward and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PushforwardFast","page":"API","title":"DifferentiationInterface.PushforwardFast","text":"PushforwardFast\n\nTrait identifying backends that support efficient pushforwards.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PushforwardSlow","page":"API","title":"DifferentiationInterface.PushforwardSlow","text":"PushforwardSlow\n\nTrait identifying backends that do not support efficient pushforwards.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ReverseOverForward","page":"API","title":"DifferentiationInterface.ReverseOverForward","text":"ReverseOverForward\n\nTraits identifying second-order backends that compute HVPs in reverse over forward mode.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ReverseOverReverse","page":"API","title":"DifferentiationInterface.ReverseOverReverse","text":"ReverseOverReverse\n\nTraits identifying second-order backends that compute HVPs in reverse over reverse mode.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.SecondDerivativeExtras","page":"API","title":"DifferentiationInterface.SecondDerivativeExtras","text":"SecondDerivativeExtras\n\nAbstract type for additional information needed by second_derivative and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.SymbolicsSparsityDetector","page":"API","title":"DifferentiationInterface.SymbolicsSparsityDetector","text":"SymbolicsSparsityDetector <: ADTypes.AbstractSparsityDetector\n\nSparsity detection algorithm based on the Symbolics.jl tracing system.\n\nCompatible with the ADTypes.jl sparsity detection framework.\n\ndanger: Danger\nThis functionality is in a package extension, and requires Symbolics.jl to be loaded.\n\nImplements\n\nADTypes.jacobian_sparsity\nADTypes.hessian_sparsity\n\nReference\n\nSparsity Programming: Automated Sparsity-Aware Optimizations in Differentiable Programming, Gowda et al. (2019)\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.TwoArgNotSupported","page":"API","title":"DifferentiationInterface.TwoArgNotSupported","text":"TwoArgNotSupported\n\nTrait identifying backends that do not support two-argument functions f!(y, x).\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.TwoArgSupported","page":"API","title":"DifferentiationInterface.TwoArgSupported","text":"TwoArgSupported\n\nTrait identifying backends that support two-argument functions f!(y, x).\n\n\n\n\n\n","category":"type"},{"location":"api/#ADTypes.mode-Tuple{SecondOrder}","page":"API","title":"ADTypes.mode","text":"mode(backend::SecondOrder)\n\nReturn the outer mode of the second-order backend.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.basis-Tuple{ADTypes.AbstractADType, AbstractArray, Any}","page":"API","title":"DifferentiationInterface.basis","text":"basis(backend, a::AbstractArray, i::CartesianIndex)\n\nConstruct the i-th stardard basis array in the vector space of a with element type eltype(a).\n\nNote\n\nIf an AD backend benefits from a more specialized basis array implementation, this function can be extended on the backend type.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.col_major-Tuple{M} where M<:(AbstractMatrix)","page":"API","title":"DifferentiationInterface.col_major","text":"col_major(A::AbstractMatrix)\n\nConstruct a column-major representation of the matrix A.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.pick_chunksize-Tuple{Integer}","page":"API","title":"DifferentiationInterface.pick_chunksize","text":"pick_chunksize(input_length)\n\nPick a reasonable chunk size for chunked derivative evaluation with an input of length input_length.\n\nThe result cannot be larger than DEFAULT_CHUNKSIZE=8.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.pullback_performance-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.pullback_performance","text":"pullback_performance(backend)\n\nReturn PullbackFast or PullbackSlow in a statically predictable way.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.pushforward_performance-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.pushforward_performance","text":"pushforward_performance(backend)\n\nReturn PushforwardFast or PushforwardSlow in a statically predictable way.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.row_major-Tuple{M} where M<:(AbstractMatrix)","page":"API","title":"DifferentiationInterface.row_major","text":"row_major(A::AbstractMatrix)\n\nConstruct a row-major representation of the matrix A.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.twoarg_support-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.twoarg_support","text":"twoarg_support(backend)\n\nReturn TwoArgSupported or TwoArgNotSupported in a statically predictable way.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"overloads/#Overloads","page":"Overloads","title":"Overloads","text":"","category":"section"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"DifferentiationInterface.jl provides a handful of operators like gradient or jacobian, each with several variants:","category":"page"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"out-of-place or in-place behavior\nwith or without primal output value\nsupport for one-argument functions y = f(x) or two-argument functions f!(y, x)","category":"page"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"While it is possible to define every operator using just pushforward and pullback, some backends have more efficient implementations of high-level operators. When they are available, we always call these backend-specific overloads.","category":"page"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"The following tables summarize all implemented overloads for each backend. The cells can have three values:","category":"page"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"❌: the operator is not overloaded because the backend does not support it\n✅: the operator is overloaded\nNA: the operator does not exist","category":"page"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"tip: Tip\nCheck marks (✅) are clickable and link to the source code.","category":"page"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"using ADTypes: AbstractADType\nusing DifferentiationInterface\nusing DifferentiationInterface: backend_str, twoarg_support, TwoArgSupported\nusing Markdown: Markdown\n\nusing Diffractor: Diffractor\nusing Enzyme: Enzyme\nusing FastDifferentiation: FastDifferentiation\nusing FiniteDiff: FiniteDiff\nusing FiniteDifferences: FiniteDifferences\nusing ForwardDiff: ForwardDiff\nusing PolyesterForwardDiff: PolyesterForwardDiff\nusing ReverseDiff: ReverseDiff\nusing Symbolics: Symbolics\nusing Tapir: Tapir\nusing Tracker: Tracker\nusing Zygote: Zygote\n\nfunction operators_and_types_f(backend::T) where {T<:AbstractADType}\n    return (\n        # (op,          types_op), \n        # (op!,         types_op!), \n        # (val_and_op,  types_val_and_op),\n        # (val_and_op!, types_val_and_op!),\n        (\n            (:derivative, (Any, T, Any, Any)),\n            (:derivative!, (Any, Any, T, Any, Any)),\n            (:value_and_derivative, (Any, T, Any, Any)),\n            (:value_and_derivative!, (Any, Any, T, Any, Any)),\n        ),\n        (\n            (:gradient, (Any, T, Any, Any)),\n            (:gradient!, (Any, Any, T, Any, Any)),\n            (:value_and_gradient, (Any, T, Any, Any)),\n            (:value_and_gradient!, (Any, Any, T, Any, Any)),\n        ),\n        (\n            (:jacobian, (Any, T, Any, Any)),\n            (:jacobian!, (Any, Any, T, Any, Any)),\n            (:value_and_jacobian, (Any, T, Any, Any)),\n            (:value_and_jacobian!, (Any, Any, T, Any, Any)),\n        ),\n        (\n            (:hessian, (Any, T, Any, Any)),\n            (:hessian!, (Any, Any, T, Any, Any)),\n            (nothing, nothing),\n            (nothing, nothing),\n        ),\n        (\n            (:hvp, (Any, T, Any, Any, Any)),\n            (:hvp!, (Any, Any, T, Any, Any, Any)),\n            (nothing, nothing),\n            (nothing, nothing),\n        ),\n        (\n            (:pullback, (Any, T, Any, Any, Any)),\n            (:pullback!, (Any, Any, T, Any, Any, Any)),\n            (:value_and_pullback, (Any, T, Any, Any, Any)),\n            (:value_and_pullback!, (Any, Any, T, Any, Any, Any)),\n        ),\n        (\n            (:pushforward, (Any, T, Any, Any, Any)),\n            (:pushforward!, (Any, Any, T, Any, Any, Any)),\n            (:value_and_pushforward, (Any, T, Any, Any, Any)),\n            (:value_and_pushforward!, (Any, Any, T, Any, Any, Any)),\n        ),\n    )\nend\n\nfunction operators_and_types_f!(backend::T) where {T<:AbstractADType}\n    return (\n        (\n            (:derivative, (Any, Any, T, Any, Any)),\n            (:derivative!, (Any, Any, Any, T, Any, Any)),\n            (:value_and_derivative, (Any, Any, T, Any, Any)),\n            (:value_and_derivative!, (Any, Any, Any, T, Any, Any)),\n        ),\n        (\n            (:jacobian, (Any, Any, T, Any, Any)),\n            (:jacobian!, (Any, Any, Any, T, Any, Any)),\n            (:value_and_jacobian, (Any, Any, T, Any, Any)),\n            (:value_and_jacobian!, (Any, Any, Any, T, Any, Any)),\n        ),\n        (\n            (:pullback, (Any, Any, T, Any, Any, Any)),\n            (:pullback!, (Any, Any, Any, T, Any, Any, Any)),\n            (:value_and_pullback, (Any, Any, T, Any, Any, Any)),\n            (:value_and_pullback!, (Any, Any, Any, T, Any, Any, Any)),\n        ),\n        (\n            (:pushforward, (Any, Any, T, Any, Any, Any)),\n            (:pushforward!, (Any, Any, Any, T, Any, Any, Any)),\n            (:value_and_pushforward, (Any, Any, T, Any, Any, Any)),\n            (:value_and_pushforward!, (Any, Any, Any, T, Any, Any, Any)),\n        ),\n    )\nend\n\nfunction method_overloaded(operator::Symbol, argtypes, ext::Module)\n    f = @eval DifferentiationInterface.$operator\n    ms = methods(f, argtypes, ext)\n\n    n = length(ms)\n    n == 0 && return \"❌\"\n    n == 1 && return \"[✅]($(Base.url(only(ms))))\"\n    return \"[✅]($(Base.url(first(ms))))\" # Optional TODO: return all URLs?\nend\n\nfunction print_overload_table(io::IO, operators_and_types, ext::Module)\n    println(io, \"| Operator | `op` | `op!` | `value_and_op` | `value_and_op!` |\")\n    println(io, \"|:---------|:----:|:-----:|:--------------:|:---------------:|\")\n    for operator_variants in operators_and_types\n        opname = first(first(operator_variants))\n        print(io, \"| `$opname` |\")\n        for (op, type_signature) in operator_variants\n            if isnothing(op)\n                print(io, \"NA\")\n            else\n                print(io, method_overloaded(op, type_signature, ext))\n            end\n            print(io, '|')\n        end\n        println(io)\n    end\nend\n\nfunction print_overloads(backend, ext::Symbol)\n    io = IOBuffer()\n    ext = Base.get_extension(DifferentiationInterface, ext)\n\n    println(io, \"#### One-argument functions `y = f(x)`\")\n    println(io)\n    print_overload_table(io, operators_and_types_f(backend), ext)\n\n    println(io, \"#### Two-argument functions `f!(y, x)`\")\n    println(io)\n    if twoarg_support(backend) == TwoArgSupported()\n        print_overload_table(io, operators_and_types_f!(backend), ext)\n    else\n        println(io, \"Backend doesn't support mutating functions.\")\n    end\n\n    return Markdown.parse(String(take!(io)))\nend","category":"page"},{"location":"overloads/#Diffractor-(forward/reverse)","page":"Overloads","title":"Diffractor (forward/reverse)","text":"","category":"section"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"print_overloads(AutoDiffractor(), :DifferentiationInterfaceDiffractorExt) # hide","category":"page"},{"location":"overloads/#Enzyme-(forward)","page":"Overloads","title":"Enzyme (forward)","text":"","category":"section"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"print_overloads(AutoEnzyme(; mode=Enzyme.Forward), :DifferentiationInterfaceEnzymeExt) # hide","category":"page"},{"location":"overloads/#Enzyme-(reverse)","page":"Overloads","title":"Enzyme (reverse)","text":"","category":"section"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"print_overloads(AutoEnzyme(; mode=Enzyme.Reverse), :DifferentiationInterfaceEnzymeExt) # hide","category":"page"},{"location":"overloads/#FastDifferentiation-(symbolic)","page":"Overloads","title":"FastDifferentiation (symbolic)","text":"","category":"section"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"print_overloads(AutoFastDifferentiation(), :DifferentiationInterfaceFastDifferentiationExt) # hide","category":"page"},{"location":"overloads/#FiniteDiff-(forward)","page":"Overloads","title":"FiniteDiff (forward)","text":"","category":"section"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"print_overloads(AutoFiniteDiff(), :DifferentiationInterfaceFiniteDiffExt) # hide","category":"page"},{"location":"overloads/#FiniteDifferences-(forward)","page":"Overloads","title":"FiniteDifferences (forward)","text":"","category":"section"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"print_overloads(AutoFiniteDifferences(; fdm=FiniteDifferences.central_fdm(3, 1)), :DifferentiationInterfaceFiniteDifferencesExt) # hide","category":"page"},{"location":"overloads/#ForwardDiff-(forward)","page":"Overloads","title":"ForwardDiff (forward)","text":"","category":"section"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"print_overloads(AutoForwardDiff(), :DifferentiationInterfaceForwardDiffExt) # hide","category":"page"},{"location":"overloads/#PolyesterForwardDiff-(forward)","page":"Overloads","title":"PolyesterForwardDiff (forward)","text":"","category":"section"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"print_overloads(AutoPolyesterForwardDiff(; chunksize=1), :DifferentiationInterfacePolyesterForwardDiffExt) # hide","category":"page"},{"location":"overloads/#ReverseDiff-(reverse)","page":"Overloads","title":"ReverseDiff (reverse)","text":"","category":"section"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"print_overloads(AutoReverseDiff(), :DifferentiationInterfaceReverseDiffExt) # hide","category":"page"},{"location":"overloads/#Symbolics-(symbolic)","page":"Overloads","title":"Symbolics (symbolic)","text":"","category":"section"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"print_overloads(AutoSymbolics(), :DifferentiationInterfaceSymbolicsExt) # hide","category":"page"},{"location":"overloads/#Tapir-(reverse)","page":"Overloads","title":"Tapir (reverse)","text":"","category":"section"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"print_overloads(AutoTapir(), :DifferentiationInterfaceTapirExt) # hide","category":"page"},{"location":"overloads/#Tracker-(reverse)","page":"Overloads","title":"Tracker (reverse)","text":"","category":"section"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"print_overloads(AutoTracker(), :DifferentiationInterfaceTrackerExt) # hide","category":"page"},{"location":"overloads/#Zygote-(reverse)","page":"Overloads","title":"Zygote (reverse)","text":"","category":"section"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"print_overloads(AutoZygote(), :DifferentiationInterfaceZygoteExt) # hide","category":"page"},{"location":"overloads/","page":"Overloads","title":"Overloads","text":"","category":"page"},{"location":"operators/#Operators","page":"Operators","title":"Operators","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"DifferentiationInterface.jl is based on two concepts: operators and backends. This page is about the former, check out that page to learn about the latter.","category":"page"},{"location":"operators/#List-of-operators","page":"Operators","title":"List of operators","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"Given a function f(x) = y, there are several differentiation operators available. The terminology depends on:","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"the type and shape of the input x\nthe type and shape of the output y\nthe order of differentiation","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"Below we list and describe all the operators we support.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"tip: Tip\nSee the book The Elements of Differentiable Programming for details on these concepts.","category":"page"},{"location":"operators/#High-level-operators","page":"Operators","title":"High-level operators","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"These operators are computed using only the input x.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"operator order input  x output   y operator result type operator result shape\nderivative 1 Number Number or AbstractArray same as y size(y)\nsecond_derivative 2 Number Number or AbstractArray same as y size(y)\ngradient 1 AbstractArray Number same as x size(x)\njacobian 1 AbstractArray AbstractArray AbstractMatrix (length(y), length(x))\nhessian 2 AbstractArray Number AbstractMatrix (length(x), length(x))","category":"page"},{"location":"operators/#Low-level-operators","page":"Operators","title":"Low-level operators","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"These operators are computed using the input x and a \"seed\" v, which lives either","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"in the same space as x (we call it dx)\nor in the same space as y (we call it dy)","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"operator order input  x output   y seed v operator result type operator result shape\npushforward (or JVP) 1 Any Any dx same as y size(y)\npullback (or VJP) 1 Any Any dy same as x size(x)\nhvp 2 AbstractArray Number dx same as x size(x)","category":"page"},{"location":"operators/#Variants","page":"Operators","title":"Variants","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"Several variants of each operator are defined.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"out-of-place in-place out-of-place + primal in-place + primal\nderivative derivative! value_and_derivative value_and_derivative!\nsecond_derivative second_derivative! NA NA\ngradient gradient! value_and_gradient value_and_gradient!\nhessian hessian! NA NA\njacobian jacobian! value_and_jacobian value_and_jacobian!\npushforward pushforward! value_and_pushforward value_and_pushforward!\npullback pullback! value_and_pullback value_and_pullback!\nhvp hvp! NA NA","category":"page"},{"location":"operators/#Mutation-and-signatures","page":"Operators","title":"Mutation and signatures","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"We support two types of functions:","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"one-argument functions f(x) = y\ntwo-argument functions f!(y, x) = nothing","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"The same operators are defined for both cases, but they have different signatures (they take different arguments):","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"signature out-of-place in-place\nf(x) operator(f,     backend, x, [v], [extras]) operator!(f,     result, backend, x, [v], [extras])\nf!(y, x) operator(f!, y, backend, x, [v], [extras]) operator!(f!, y, result, backend, x, [v], [extras])","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"warning: Warning\nThe positional arguments between f/f! and backend are always mutated. This convention holds regardless of the bang ! in the operator name. In particular, for two-argument functions f!(y, x), every variant of every operator will mutate y.","category":"page"},{"location":"operators/#Operators-Preparation","page":"Operators","title":"Preparation","text":"","category":"section"},{"location":"operators/#Principle","page":"Operators","title":"Principle","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"In many cases, AD can be accelerated if the function has been called at least once (e.g. to record a tape) or if some cache objects are provided. This preparation procedure is backend-specific, but we expose a common syntax to achieve it.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"operator preparation function (different point) preparation function (same point)\nderivative prepare_derivative -\ngradient prepare_gradient -\njacobian prepare_jacobian -\nsecond_derivative prepare_second_derivative -\nhessian prepare_hessian -\npushforward prepare_pushforward prepare_pushforward_same_point\npullback prepare_pullback prepare_pullback_same_point\nhvp prepare_hvp prepare_hvp_same_point","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"In addition, the preparation syntax depends on the number of arguments accepted by the function.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"signature preparation signature\nf(x) prepare_operator(f,     backend, x, [v])\nf!(y, x) prepare_operator(f!, y, backend, x, [v])","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"Preparation creates an object called extras which contains the the necessary information to speed up an operator and its variants. The idea is that you prepare only once, which can be costly, but then call the operator several times while reusing the same extras.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"operator(f, backend, x, [v])  # slow because it includes preparation\noperator(f, backend, x, [v], extras)  # fast because it skips preparation","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"warning: Warning\nThe extras object is always mutated when given to an operator, even though it is the last argument. This convention holds regardless of the bang ! in the operator name.","category":"page"},{"location":"operators/#Reusing-preparation","page":"Operators","title":"Reusing preparation","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"Deciding whether it is safe to reuse the results of preparation is not easy. Here are the general rules that we strive to implement:","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":" different point same point\nthe output extras of... prepare_operator(f, b, x) prepare_operator_same_point(f, b, x, v)\ncan be used in... operator(f, b, other_x, extras) operator(f, b, x, other_v, extras)\nprovided that... other_x has the same type and shape as x other_v has the same type and shape as v","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"These rules hold for the majority of backends, but there are some exceptions: see this page to know more.","category":"page"},{"location":"operators/#Second-order","page":"Operators","title":"Second order","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"For second-order operators, there are two options: use a single backend or combine two of them within the SecondOrder struct.","category":"page"},{"location":"operators/#Single-backend","page":"Operators","title":"Single backend","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"Some backends natively support a set of second-order operators (typically only the hessian). In that case, it can be advantageous to use the backend on its own. If the operator is not supported natively, we will fall back on SecondOrder(backend, backend) (see below).","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"warning: Warning\nWhenever the fallback on SecondOrder(backend, backend) occurs, the results of any preparation will be discarded.","category":"page"},{"location":"operators/#Combining-backends","page":"Operators","title":"Combining backends","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"In general, you can use SecondOrder to combine different backends.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"backend = SecondOrder(outer_backend, inner_backend)","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"The inner backend will be called first, and the outer backend will differentiate the generated code.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"There are many possible backend combinations, a lot of which will fail. Usually, the most efficient approach for Hessians is forward-over-reverse, i.e. a forward-mode outer backend and a reverse-mode inner backend.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"danger: Danger\nSecondOrder backends do not support first-order operators.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"warning: Warning\nPreparation does not yet work for the inner differentiation step of a SecondOrder, only the outer differentiation is prepared.","category":"page"},{"location":"operators/#Sparsity","page":"Operators","title":"Sparsity","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"When computing sparse Jacobians or Hessians, it is possible to take advantage of their sparsity pattern to speed things up. For this to work, three ingredients are needed (read this survey to understand why):","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"An underlying (dense) backend\nA sparsity pattern detector like TracerSparsityDetector from SparseConnectivityTracer.jl\nA coloring algorithm like GreedyColoringAlgorithm from SparseMatrixColorings.jl","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"These ingredients can be combined within the AutoSparse wrapper, which DifferentiationInterface.jl re-exports. Note that for sparse Hessians, you need to put the SecondOrder backend inside AutoSparse, and not the other way around.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"The preparation step of jacobian or hessian with an AutoSparse backend can be long, because it needs to detect the sparsity pattern and color the resulting sparse matrix. But after preparation, the more zeros are present in the matrix, the greater the speedup will be compared to dense differentiation.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"danger: Danger\nAutoSparse backends only support operators jacobian and hessian (as well as their variants).","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"warning: Warning\nThe result of preparation for an AutoSparse backend cannot be reused if the sparsity pattern changes.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"info: Info\nSymbolic backends have built-in sparsity handling, so AutoSparse(AutoSymbolics()) and AutoSparse(AutoFastDifferentiation()) do not need additional configuration for pattern detection or coloring. However they still benefit from preparation.","category":"page"},{"location":"operators/#Going-further","page":"Operators","title":"Going further","text":"","category":"section"},{"location":"operators/#Non-standard-types","page":"Operators","title":"Non-standard types","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"The package is thoroughly tested with inputs and outputs of the following types: Float64, Vector{Float64} and Matrix{Float64}. We also expect it to work on most kinds of Number and AbstractArray variables. Beyond that, you are in uncharted territory. We voluntarily keep the type annotations minimal, so that passing more complex objects or custom structs might work with some backends, but we make no guarantees about that.","category":"page"},{"location":"operators/#Multiple-inputs/outputs","page":"Operators","title":"Multiple inputs/outputs","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"Restricting the API to one input and one output has many coding advantages, but it is not very flexible. If you need more than that, use ComponentArrays.jl to wrap several objects inside a single ComponentVector.","category":"page"},{"location":"operators/#Batched-evaluation","page":"Operators","title":"Batched evaluation","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"This is not supported at the moment, but we plan to allow several seeds at once in low-level operators (similar to the chunking in ForwardDiff.jl or the batches in Enzyme.jl).","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"","category":"page"},{"location":"preparation/#Preparation","page":"Preparation","title":"Preparation","text":"","category":"section"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"Preparation is a backend-specific procedure which involves some subtleties. Here we list the broad principles of preparation for each backend where it is nontrivial.","category":"page"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"The following is not part of the public API.","category":"page"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"warning: Warning\nThis page may become outdated, in which case you should refer to the source code as the ground truth.","category":"page"},{"location":"preparation/#ChainRulesCore","page":"Preparation","title":"ChainRulesCore","text":"","category":"section"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"For pullback, same-point preparation runs the forward sweep and returns the pullback closure.","category":"page"},{"location":"preparation/#Enzyme","page":"Preparation","title":"Enzyme","text":"","category":"section"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"In forward mode, for gradient and jacobian","category":"page"},{"location":"preparation/#FastDifferentiation","page":"Preparation","title":"FastDifferentiation","text":"","category":"section"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"Preparation generates an executable function from the symbolic expression of the differentiated function.","category":"page"},{"location":"preparation/#FiniteDiff","page":"Preparation","title":"FiniteDiff","text":"","category":"section"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"Whenever possible, preparation creates a cache object.","category":"page"},{"location":"preparation/#ForwardDiff","page":"Preparation","title":"ForwardDiff","text":"","category":"section"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"Wherever possible, preparation creates a config with all the necessary memory to use as buffer. For pushforward, preparation allocates the necessary space for Dual number computations.","category":"page"},{"location":"preparation/#ReverseDiff","page":"Preparation","title":"ReverseDiff","text":"","category":"section"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"Wherever possible, preparation records a tape of the function's execution.","category":"page"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"warning: Warning\nThis tape is specific to the control flow inside the function, and cannot be reused if the control flow is value-dependent (like if x[1] > 0).","category":"page"},{"location":"preparation/#Symbolics","page":"Preparation","title":"Symbolics","text":"","category":"section"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"Preparation generates an executable function from the symbolic expression of the differentiated function.","category":"page"},{"location":"preparation/#Tapir","page":"Preparation","title":"Tapir","text":"","category":"section"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"For pullback, preparation builds the reverse rule of the function.","category":"page"},{"location":"preparation/#Tracker","page":"Preparation","title":"Tracker","text":"","category":"section"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"For pullback, same-point preparation runs the forward sweep and returns the pullback closure at x.","category":"page"},{"location":"preparation/#Zygote","page":"Preparation","title":"Zygote","text":"","category":"section"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"For pullback, same-point preparation runs the forward sweep and returns the pullback closure at x.","category":"page"},{"location":"preparation/","page":"Preparation","title":"Preparation","text":"","category":"page"},{"location":"tutorial1/#Basics","page":"Basics","title":"Basics","text":"","category":"section"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"We present the main features of DifferentiationInterface.jl.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"using DifferentiationInterface","category":"page"},{"location":"tutorial1/#Computing-a-gradient","page":"Basics","title":"Computing a gradient","text":"","category":"section"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"A common use case of automatic differentiation (AD) is optimizing real-valued functions with first- or second-order methods. Let's define a simple objective and a random input vector","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"f(x) = sum(abs2, x)\n\nx = collect(1.0:5.0)","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"To compute its gradient, we need to choose a \"backend\", i.e. an AD package to call under the hood. Most backend types are defined by ADTypes.jl and re-exported by DifferentiationInterface.jl.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"ForwardDiff.jl is very generic and efficient for low-dimensional inputs, so it's a good starting point:","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"import ForwardDiff\n\nbackend = AutoForwardDiff()","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"tip: Tip\nTo avoid name conflicts, load AD packages with import instead of using. Indeed, most AD packages also export operators like gradient and jacobian, but you only want to use the ones from DifferentiationInterface.jl.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Now you can use the following syntax to compute the gradient:","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"gradient(f, backend, x)","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Was that fast? BenchmarkTools.jl helps you answer that question.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"using BenchmarkTools\n\n@benchmark gradient($f, $backend, $x)","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Not bad, but you can do better.","category":"page"},{"location":"tutorial1/#Overwriting-a-gradient","page":"Basics","title":"Overwriting a gradient","text":"","category":"section"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Since you know how much space your gradient will occupy (the same as your input x), you can pre-allocate that memory and offer it to AD. Some backends get a speed boost from this trick.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"grad = similar(x)\ngradient!(f, grad, backend, x)\ngrad  # has been mutated","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"The bang indicates that one of the arguments of gradient! might be mutated. More precisely, our convention is that every positional argument between the function and the backend is mutated (and the extras too, see below).","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"@benchmark gradient!($f, _grad, $backend, $x) evals=1 setup=(_grad=similar($x))","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"For some reason the in-place version is not much better than your first attempt. However, it makes fewer allocations, thanks to the gradient vector you provided. Don't worry, you can get even more performance.","category":"page"},{"location":"tutorial1/#Preparing-for-multiple-gradients","page":"Basics","title":"Preparing for multiple gradients","text":"","category":"section"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Internally, ForwardDiff.jl creates some data structures to keep track of things. These objects can be reused between gradient computations, even on different input values. We abstract away the preparation step behind a backend-agnostic syntax:","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"extras = prepare_gradient(f, backend, randn(eltype(x), size(x)))","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"You don't need to know what this object is, you just need to pass it to the gradient operator. Note that preparation does not depend on the actual components of the vector x, just on its type and size. You can thus reuse the extras for different values of the input.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"grad = similar(x)\ngradient!(f, grad, backend, x, extras)\ngrad  # has been mutated","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Preparation makes the gradient computation much faster, and (in this case) allocation-free.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"@benchmark gradient!($f, _grad, $backend, $x, _extras) evals=1 setup=(\n    _grad=similar($x);\n    _extras=prepare_gradient($f, $backend, $x)\n)","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Beware that the extras object is nearly always mutated by differentiation operators, even though it is given as the last positional argument.","category":"page"},{"location":"tutorial1/#Switching-backends","page":"Basics","title":"Switching backends","text":"","category":"section"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"The whole point of DifferentiationInterface.jl is that you can easily experiment with different AD solutions. Typically, for gradients, reverse mode AD might be a better fit, so let's try the state-of-the-art Enzyme.jl!","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"import Enzyme\n\nbackend2 = AutoEnzyme()","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Once the backend is created, things run smoothly with exactly the same syntax as before:","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"gradient(f, backend2, x)","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"And you can run the same benchmarks to see what you gained (although such a small input may not be realistic):","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"@benchmark gradient!($f, _grad, $backend2, $x, _extras) evals=1 setup=(\n    _grad=similar($x);\n    _extras=prepare_gradient($f, $backend2, $x)\n)","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"In short, DifferentiationInterface.jl allows for easy testing and comparison of AD backends. If you want to go further, check out the documentation of DifferentiationInterfaceTest.jl. This related package provides benchmarking utilities to compare backends and help you select the one that is best suited for your problem.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"","category":"page"},{"location":"tutorial2/#Sparsity","page":"Sparsity","title":"Sparsity","text":"","category":"section"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"We present sparsity handling with DifferentiationInterface.jl.","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"using BenchmarkTools\nusing DifferentiationInterface\nimport ForwardDiff, Zygote","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"Sparse AD is very useful when Jacobian or Hessian matrices have a lot of zeros. So let us write functions that satisfy this property.","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"f_sparse_vector(x::AbstractVector) = diff(x .^ 2) + diff(reverse(x .^ 2))\nf_sparse_scalar(x::AbstractVector) = sum(f_sparse_vector(x) .^ 2)\nnothing  # hide","category":"page"},{"location":"tutorial2/#Dense-backends","page":"Sparsity","title":"Dense backends","text":"","category":"section"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"When we use the jacobian or hessian operator with a dense backend, we get a dense matrix with plenty of zeros.","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"dense_first_order_backend = AutoForwardDiff()\nJ_dense = jacobian(f_sparse_vector, dense_first_order_backend, float.(1:10))","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"dense_second_order_backend = SecondOrder(AutoForwardDiff(), AutoZygote())\nH_dense = hessian(f_sparse_scalar, dense_second_order_backend, float.(1:10))","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"The results are correct but the procedure is very slow. By using a sparse backend, we can get the runtime to increase with the number of nonzero elements, instead of the total number of elements.","category":"page"},{"location":"tutorial2/#Sparse-backends","page":"Sparsity","title":"Sparse backends","text":"","category":"section"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"Recipe to create a sparse backend: combine a dense backend, a sparsity detector and a coloring algorithm inside AutoSparse. The following are reasonable defaults:","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"using SparseConnectivityTracer: TracerSparsityDetector\nusing SparseMatrixColorings: GreedyColoringAlgorithm\n\nsparse_first_order_backend = AutoSparse(\n    AutoForwardDiff();\n    sparsity_detector=TracerSparsityDetector(),\n    coloring_algorithm=GreedyColoringAlgorithm(),\n)\n\nsparse_second_order_backend = AutoSparse(\n    SecondOrder(AutoForwardDiff(), AutoZygote());\n    sparsity_detector=TracerSparsityDetector(),\n    coloring_algorithm=GreedyColoringAlgorithm(),\n)\nnothing  # hide","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"Now the resulting matrices are sparse:","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"jacobian(f_sparse_vector, sparse_first_order_backend, float.(1:10))","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"hessian(f_sparse_scalar, sparse_second_order_backend, float.(1:10))","category":"page"},{"location":"tutorial2/#Sparse-preparation","page":"Sparsity","title":"Sparse preparation","text":"","category":"section"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"In the examples above, we didn't use preparation. Sparse preparation is more costly than dense preparation, but it is even more essential. Indeed, once preparation is done, sparse differentiation is much faster than dense differentiation, because it makes fewer calls to the underlying function. The speedup becomes very visible in large dimensions.","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"n = 1000\njac_extras_dense = prepare_jacobian(f_sparse_vector, dense_first_order_backend, randn(n));\njac_extras_sparse = prepare_jacobian(f_sparse_vector, sparse_first_order_backend, randn(n));\nnothing  # hide","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"@benchmark jacobian($f_sparse_vector, $dense_first_order_backend, $(randn(n)), $jac_extras_dense) evals=1","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"@benchmark jacobian($f_sparse_vector, $sparse_first_order_backend, $(randn(n)), $jac_extras_sparse) evals=1","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"","category":"page"},{"location":"backends/#Backends","page":"Backends","title":"Backends","text":"","category":"section"},{"location":"backends/","page":"Backends","title":"Backends","text":"DifferentiationInterface.jl is based on two concepts: operators and backends. This page is about the latter, check out that page to learn about the former.","category":"page"},{"location":"backends/#List-of-backends","page":"Backends","title":"List of backends","text":"","category":"section"},{"location":"backends/","page":"Backends","title":"Backends","text":"We support all dense backend choices from ADTypes.jl, as well as their sparse wrapper AutoSparse.","category":"page"},{"location":"backends/","page":"Backends","title":"Backends","text":"using DifferentiationInterface\nusing DifferentiationInterface: backend_str\nimport Markdown\n\nimport Diffractor\nimport Enzyme\nimport FastDifferentiation\nimport FiniteDiff\nimport FiniteDifferences\nimport ForwardDiff\nimport PolyesterForwardDiff\nimport ReverseDiff\nimport Symbolics\nimport Tapir\nimport Tracker\nimport Zygote\n\nconst backend_examples = (\n    \"AutoDiffractor()\",\n    \"AutoEnzyme(; mode=Enzyme.Forward)\",\n    \"AutoEnzyme(; mode=Enzyme.Reverse)\",\n    \"AutoFastDifferentiation()\",\n    \"AutoFiniteDiff()\",\n    \"AutoFiniteDifferences(; fdm=FiniteDifferences.central_fdm(3, 1))\",\n    \"AutoForwardDiff()\",\n    \"AutoPolyesterForwardDiff(; chunksize=1)\",\n    \"AutoReverseDiff()\",\n    \"AutoSymbolics()\",\n    \"AutoTapir(; safe_mode=false)\",\n    \"AutoTracker()\",\n    \"AutoZygote()\",\n)\n\ncheckmark(x::Bool) = x ? '✅' : '❌'\nunicode_check_available(backend) = checkmark(check_available(backend))\nunicode_check_hessian(backend)   = checkmark(check_hessian(backend; verbose=false))\nunicode_check_twoarg(backend)    = checkmark(check_twoarg(backend))\n\nio = IOBuffer()\n\n# Table header \nprintln(io, \"| Backend | Availability | Two-argument functions | Hessian support | Example |\")\nprintln(io, \"|:--------|:------------:|:----------------------:|:---------------:|:--------|\")\n\nfor example in backend_examples\n    b = eval(Meta.parse(example)) # backend\n    join(io, [backend_str(b), unicode_check_available(b), unicode_check_twoarg(b), unicode_check_hessian(b), \"`$example`\"], '|')\n    println(io, '|' )\nend\nbackend_table = Markdown.parse(String(take!(io)))","category":"page"},{"location":"backends/","page":"Backends","title":"Backends","text":"backend_table #hide","category":"page"},{"location":"backends/","page":"Backends","title":"Backends","text":"danger: Compatibility with Julia 1.6\nAs of version 0.3.4, DifferentiationInterface.jl is compatible with Julia 1.6, the Long Term Support (LTS) version of the language. However, we were only able to test the following backends on LTS:FiniteDifferences.jl\nForwardDiff.jl\nReverseDiff.jl\nTracker.jl\nZygote.jlWe strongly recommend that users upgrade to Julia 1.10, where all backends are tested.","category":"page"},{"location":"backends/#Checks","page":"Backends","title":"Checks","text":"","category":"section"},{"location":"backends/#Availability","page":"Backends","title":"Availability","text":"","category":"section"},{"location":"backends/","page":"Backends","title":"Backends","text":"You can use check_available to verify whether a given backend is loaded.","category":"page"},{"location":"backends/#Support-for-two-argument-functions","page":"Backends","title":"Support for two-argument functions","text":"","category":"section"},{"location":"backends/","page":"Backends","title":"Backends","text":"All backends are compatible with one-argument functions f(x) = y. Only some are compatible with two-argument functions f!(y, x) = nothing. You can use check_twoarg to verify this compatibility.","category":"page"},{"location":"backends/#Support-for-Hessian","page":"Backends","title":"Support for Hessian","text":"","category":"section"},{"location":"backends/","page":"Backends","title":"Backends","text":"Only some backends are able to compute Hessians. You can use check_hessian to verify this feature (beware that it will try to compute a small Hessian, so it is not instantaneous like the other checks).","category":"page"},{"location":"backends/#Backend-switch","page":"Backends","title":"Backend switch","text":"","category":"section"},{"location":"backends/","page":"Backends","title":"Backends","text":"The wrapper DifferentiateWith allows you to switch between backends. It takes a function f and specifies that f should be differentiated with the backend of your choice, instead of whatever other backend the code is trying to use. In other words, when someone tries to differentiate dw = DifferentiateWith(f, backend1) with backend2, then backend1 steps in and backend2 does nothing. At the moment, DifferentiateWith only works when backend2 supports ChainRules.jl.","category":"page"},{"location":"backends/#Defining-your-own","page":"Backends","title":"Defining your own","text":"","category":"section"},{"location":"backends/","page":"Backends","title":"Backends","text":"To work with DifferentiationInterface.jl, a new AD system would need to create an object subtyping ADTypes.AbstractADType. In addition, some low-level operators would need to be defined at the very least:","category":"page"},{"location":"backends/","page":"Backends","title":"Backends","text":"backend subtype pushforward necessary pullback necessary\nADTypes.ForwardMode yes no\nADTypes.ReverseMode no yes\nADTypes.SymbolicMode yes yes","category":"page"},{"location":"backends/","page":"Backends","title":"Backends","text":"Every backend we support corresponds to a package extension of DifferentiationInterface.jl (located in the ext subfolder). If you need to implement your own backend, take a look in there for inspiration, or reach out to us in the GitHub issues.","category":"page"},{"location":"backends/","page":"Backends","title":"Backends","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: DifferentiationInterface Logo)","category":"page"},{"location":"#DifferentiationInterface","page":"Home","title":"DifferentiationInterface","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Build Status) (Image: Coverage) (Image: Code Style: Blue) (Image: DOI)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Package Docs\nDifferentiationInterface (Image: Stable)     (Image: Dev)\nDifferentiationInterfaceTest (Image: Stable) (Image: Dev)","category":"page"},{"location":"","page":"Home","title":"Home","text":"An interface to various automatic differentiation (AD) backends in Julia.","category":"page"},{"location":"#Goal","page":"Home","title":"Goal","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides a backend-agnostic syntax to differentiate functions of the following types:","category":"page"},{"location":"","page":"Home","title":"Home","text":"one-argument functions (allocating): f(x) = y\ntwo-argument functions (mutating): f!(y, x) = nothing","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"First- and second-order operators (gradients, Jacobians, Hessians and more)\nIn-place and out-of-place differentiation\nPreparation mechanism (e.g. to create a config or tape)\nBuilt-in sparsity handling\nThorough validation on standard inputs and outputs (numbers, vectors, matrices)\nTesting and benchmarking utilities accessible to users with DifferentiationInterfaceTest","category":"page"},{"location":"#Compatibility","page":"Home","title":"Compatibility","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We support all of the backends defined by ADTypes.jl v1.0:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ChainRulesCore.jl\nDiffractor.jl\nEnzyme.jl\nFastDifferentiation.jl\nFiniteDiff.jl\nFiniteDifferences.jl\nForwardDiff.jl\nPolyesterForwardDiff.jl\nReverseDiff.jl\nSymbolics.jl\nTapir.jl\nTracker.jl\nZygote.jl","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the stable version of the package, run the following code in a Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\n\nPkg.add(\"DifferentiationInterface\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"To install the development version, run this instead:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\n\nPkg.add(\n    url=\"https://github.com/gdalle/DifferentiationInterface.jl\",\n    subdir=\"DifferentiationInterface\"\n)","category":"page"},{"location":"#Example","page":"Home","title":"Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using DifferentiationInterface\nimport ForwardDiff, Enzyme, Zygote  # AD backends you want to use \n\nf(x) = sum(abs2, x)\n\nx = [1.0, 2.0]\n\nvalue_and_gradient(f, AutoForwardDiff(), x) # returns (5.0, [2.0, 4.0]) with ForwardDiff.jl\nvalue_and_gradient(f, AutoEnzyme(),      x) # returns (5.0, [2.0, 4.0]) with Enzyme.jl\nvalue_and_gradient(f, AutoZygote(),      x) # returns (5.0, [2.0, 4.0]) with Zygote.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"To improve your performance by up to several orders of magnitude compared to this example, take a look at the DifferentiationInterface tutorial and its section on operator preparation.","category":"page"},{"location":"#Related-packages","page":"Home","title":"Related packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"AbstractDifferentiation.jl is the original inspiration for DifferentiationInterface.jl.\nAutoDiffOperators.jl is an attempt to bridge ADTypes.jl with AbstractDifferentiation.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
